{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with open(r'full_text.yaml') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    question_answer_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in question_answer_data:\n",
    "    dat = []\n",
    "    answer = {}\n",
    "    dat.append(question_answer_data[i]['context'])\n",
    "    dat.append(question_answer_data[i]['question'])\n",
    "    answer['text'] = question_answer_data[i]['answer']\n",
    "    answer_start = question_answer_data[i]['context'].find(answer['text']);\n",
    "    answer[\"answer_start\"] = answer_start\n",
    "    answer[\"answer_end\"] = answer_start + len(question_answer_data[i]['answer'])\n",
    "    dat.append(answer)\n",
    "    data.append(dat)\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "val_contexts = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "train_contexts = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "\n",
    "n = 6\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if i < n:\n",
    "        val_contexts.append(data[i][0])\n",
    "        val_questions.append(data[i][1])\n",
    "        val_answers.append(data[i][2])\n",
    "    else:\n",
    "        train_contexts.append(data[i][0])\n",
    "        train_questions.append(data[i][1])\n",
    "        train_answers.append(data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = {}\n",
    "\n",
    "def count_the_words(counts, string):\n",
    "    words = string.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "for i in train_contexts:\n",
    "     count_the_words(count_words, i)\n",
    "for i in val_contexts:\n",
    "     count_the_words(count_words, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAE/CAYAAADlmNKjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATbElEQVR4nO3de7BlZ13m8e9DmhkwQUNIT1cmF5vBVipaGqVFkMxUkEG51FTiiAFESSxmWstkEBXHzJQjTJVUxXFmqLJmCIYBEmOMhFEkSgRTQaS5BOiGXDokKbtMZ5KYS4MQidxM8ps/9nvIpnM657L3r8853d9P1a6z9rvXet937b3Wfva71j5rp6qQJEnz9YS17oAkSYcjA1aSpAYGrCRJDQxYSZIaGLCSJDUwYCVJarBprTsAcPzxx9fWrVvXuhuSJK3I7t27P1dVmxd7bF0E7NatW9m1a9dad0OSpBVJcsfBHvMQsSRJDQxYSZIaGLCSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqsC4u9i9J0oG2XvC+lnr3XfjSlnoP5AhWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVIDA1aSpAYGrCRJDQxYSZIaGLCSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSgyUDNsnJSf4yyWeT3JzkF0f5cUmuSfLX4+9TR3mS/E6SvUluTPID3SshSdJ6s5wR7EPAr1TVqcBzgPOSnApcAFxbVduAa8d9gBcD28ZtB3DR3HstSdI6t2TAVtU9VfXpMf0l4BbgROBM4NIx26XAWWP6TOD3auI64NgkJ8y955IkrWMrOgebZCvw/cAngC1Vdc946F5gy5g+EbhzarG7RpkkSUeMZQdskmOAPwJeV1V/P/1YVRVQK2k4yY4ku5Ls2r9//0oWlSRp3VtWwCZ5IpNwvbyq/ngU37dw6Hf8vX+U3w2cPLX4SaPsm1TVxVW1vaq2b968ebX9lyRpXVrOt4gDvB24par+59RDVwHnjOlzgPdOlb96fJv4OcADU4eSJUk6ImxaxjzPA34GuCnJ9aPsPwMXAlcmeQ1wB3D2eOxq4CXAXuDLwM/OtceSJG0ASwZsVX0EyEEefsEi8xdw3oz9kiRpQ/NKTpIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVIDA1aSpAYGrCRJDQxYSZIaGLCSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVIDA1aSpAYGrCRJDQxYSZIaGLCSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSgyUDNsk7ktyfZM9U2RuT3J3k+nF7ydRj/ynJ3iS3Jfmxro5LkrSeLWcEewnwokXK31xVp43b1QBJTgVeAXz3WOYtSY6aV2clSdoolgzYqvow8HfLrO9M4A+r6mtVdTuwF3j2DP2TJGlDmuUc7PlJbhyHkJ86yk4E7pya565R9hhJdiTZlWTX/v37Z+iGJEnrz2oD9iLgGcBpwD3A/1hpBVV1cVVtr6rtmzdvXmU3JElan1YVsFV1X1U9XFWPAG/j0cPAdwMnT8160iiTJOmIsqqATXLC1N0fBxa+YXwV8Iok/zTJ04FtwCdn66IkSRvPpqVmSHIFcAZwfJK7gDcAZyQ5DShgH/BzAFV1c5Irgc8CDwHnVdXDPV2XJGn9WjJgq+qVixS//XHmfxPwplk6JUnSRueVnCRJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVIDA1aSpAYGrCRJDQxYSZIaGLCSJDUwYCVJamDASpLUwICVJKmBAStJUoNNa90BSdLGsvWC9829zn0XvnTuda41R7CSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVIDA1aSpAYGrCRJDZYM2CTvSHJ/kj1TZccluSbJX4+/Tx3lSfI7SfYmuTHJD3R2XpKk9Wo5I9hLgBcdUHYBcG1VbQOuHfcBXgxsG7cdwEXz6aYkSRvLkgFbVR8G/u6A4jOBS8f0pcBZU+W/VxPXAccmOWFenZUkaaNY7TnYLVV1z5i+F9gypk8E7pya765R9hhJdiTZlWTX/v37V9kNSZLWp5m/5FRVBdQqlru4qrZX1fbNmzfP2g1JktaV1QbsfQuHfsff+0f53cDJU/OdNMokSTqirDZgrwLOGdPnAO+dKn/1+Dbxc4AHpg4lS5J0xNi01AxJrgDOAI5PchfwBuBC4MokrwHuAM4es18NvATYC3wZ+NmGPkuStO4tGbBV9cqDPPSCReYt4LxZOyVJ0kbnlZwkSWpgwEqS1MCAlSSpgQErSVIDA1aSpAYGrCRJDQxYSZIaGLCSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNlvy5OknS+rf1gvfNvc59F7507nUeSRzBSpLUwICVJKmBAStJUgMDVpKkBn7JSZIa+eWjI5cjWEmSGjiClXTEcVSpQ8ERrCRJDQxYSZIaGLCSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVIDA1aSpAYGrCRJDTatdQckacHWC9439zr3XfjSudcpLYcjWEmSGhiwkiQ1MGAlSWpgwEqS1GCmLzkl2Qd8CXgYeKiqtic5DngXsBXYB5xdVV+YrZuSJG0s8xjBPr+qTquq7eP+BcC1VbUNuHbclyTpiNJxiPhM4NIxfSlwVkMbkiSta7MGbAF/kWR3kh2jbEtV3TOm7wW2zNiGJEkbzqwXmji9qu5O8s+Aa5LcOv1gVVWSWmzBEcg7AE455ZQZuyFJ0voyU8BW1d3j7/1J3gM8G7gvyQlVdU+SE4D7D7LsxcDFANu3b180hCWtPa+uJK3Oqg8RJzk6yVMWpoEfBfYAVwHnjNnOAd47ayclSdpoZhnBbgHek2Shnj+oqvcn+RRwZZLXAHcAZ8/eTUmSNpZVB2xV/Q3wfYuUfx54wSydkiRpo/NKTpIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVKDmX5wXdLa8YfQpfXNgJXmyNCTtMBDxJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1MGAlSWpgwEqS1MCAlSSpgQErSVIDf01HRwR/5UbSoeYIVpKkBo5g9U06Rnqw+GjPUaWkw5kBu0EYRpK0sXiIWJKkBo5gZ3AoD6dKkjYWR7CSJDU4LEewnq+UJK01R7CSJDUwYCVJamDASpLUwICVJKmBAStJUgMDVpKkBgasJEkNDFhJkhoYsJIkNTBgJUlqYMBKktTAgJUkqYEBK0lSAwNWkqQGBqwkSQ0MWEmSGrQFbJIXJbktyd4kF3S1I0nSetQSsEmOAv438GLgVOCVSU7taEuSpPWoawT7bGBvVf1NVX0d+EPgzKa2JElad7oC9kTgzqn7d40ySZKOCKmq+VeavAx4UVX9u3H/Z4Afqqrzp+bZAewYd78LuG3uHVna8cDnbGvdt3Mo23KdNkZbrtPGaOtwXKcDfXtVbV7sgU1NDd4NnDx1/6RR9g1VdTFwcVP7y5JkV1Vtt6313c6hbMt12hhtuU4bo63DcZ1WousQ8aeAbUmenuSfAK8ArmpqS5KkdadlBFtVDyU5H/gAcBTwjqq6uaMtSZLWo65DxFTV1cDVXfXPyaE8RH04tuU6bYy2XKeN0ZbrtHHaWpaWLzlJknSk81KJkiQ1OCwDNslrk9yS5PJD1N7WJHtmWP7BtWj3ceo9NskvjOkzkvzZCpc/N8k/n7EPq3pO1CPJ05JcP273Jrl7TH8xyWfXun9rZdbtNMnH5tWXx2njG/vzWpnHe/Ks70tr4bAMWOAXgBdW1asWCpK0nW8+DB3L5DlcrXOBmQL2SDUuM7ruVNXnq+q0qjoNeCvw5jF9GvDI2vZu46qqHz4Ezcy6P8/DY96TV2E9rMeKHHYBm+StwL8A/jzJA0kuS/JR4LIkT0ryziQ3JflMkuePZc5N8idJrkmyL8n5SX55zHNdkuOW0fRRSd6W5OYkf5HkyUn+fZJPJbkhyR8l+ZbR3tOTfHz04zdnXOVNSS4fnw7/b5KXJPmTqefjhUnes8I6LwSekeR64LeBY0bdt462Mur+jbF+e5JcnImXAduBy8cI58mzrNz4pPqhxdqf1XiN94zb68YRgVsOfB1XUN+vJnntmH5zkg+O6R8Z/f7R8bp/Osm7kxwzHt+X5LeSfBr4ySTPSPL+JLuT7EzyzBnavCjJrrE+/3VquQuTfDbJjUn++6qewEc9ZtsfbSx7PVZi7Ku7R3s7ll5i5fUleTDJm8a+e12SLaN8nvvuN0bAnds5U/tzkt8etz1jHV4+pza+YZH9avo9+ZdmqHq570vPSvJX4zX9QJITZl+rVaqqw+4G7GNyVY83AruBJ4/yX2HyL0MAzwT+H/AkJiOuvcBTgM3AA8DPj/neDLxuifa2Ag8Bp437VwI/DTxtap7fBP7DmL4KePWYPg94cJXruRUo4Hnj/juAXwVuBTaPsj8A/s0q6t0zps8Yz8dJTD6QfRw4fTx23NQyly20A3wI2D7ja/jgUu3PWP+zgJuAo4FjgJuB71/sdVxBnc8B3j2mdwKfBJ4IvAH4NeDDwNHj8V8DfmNqe/2PU/VcC2wb0z8EfHCVbf7cwmvE5N/lPgR8L/A0JldOW/iS47ErfO7eCLz+8bb9la7HCttfWKcnA3uY2s/mVd/Yrxa25/8G/PqYnsu+e6i286nXaGF//gngmrE9bGHyHnjCPNoZ9R9sv9oHHD/H9Vj0+Rrb/sd49P3v5Yz3/LW4HXYj2EVcVVVfGdOnA78PUFW3AncA3zke+8uq+lJV7Wfywv3pKL+JyQu7lNur6voxvXss8z3jk/tNwKuA7x6PPw+4YkxftpqVmnJnVX10TP/+qPsy4KeTHAs8F/jzGdv4ZFXdVVWPANfz6PPx/CSfGOv3Izy6fvN2sPZncTrwnqr6h6p6EPhj4F+y+Ou4XLuBZyX5VuBrTHb67aPerzD5ZamPjk/g5wDfPrXsuwDGqPaHgXeP+X4XeLxP4I/X5k7g7DEy/gyT1+dUJtv3V4G3J/m3wJdXsI6Lecxztor1WInXJrkBuI7JFeO2NdT3dWDhHN/0djDPffdAHdv5gU4Hrqiqh6vqPuCvgB+cc/2L7VcdFnu+vgv4HuCasd39OpMQXhNHwnnJf1jmfF+bmn5k6v4jLO95ml7+YSafhi8BzqqqG5Kcy+RT14J5/X/UgfUU8E4mHxC+ymR089CMbRy4bpuSPAl4C5OR6p1J3sjkaECHx7Tf1M5ibS37EHFV/WOS25kcEfkYcCPwfOA7gNuBa6rqlQdZfGE7fQLwxZqc35y1za8Arwd+sKq+kOQS4Ek1uRDMs4EXAC8DzmfyAWm1FnvOVrQey5XkDOBfA8+tqi8n+RAzbHePU98/1hgC8dhtrut/Gw/ldn44WOz5CnBzVT13bbr0zY6EEey0nUxGkiT5TuAUen9k4CnAPUmeuNDu8FEml4/kgPLVOCXJwsb0U8BHqupvgb9l8untnauo80tM+v54Ft7UPjdGKy9b4fJrbSdwVpJvSXI08OOjbB71vp7J4eCdwM8zGT1eBzwvyXcAJDl6bIPfpKr+Hrg9yU+O+ZLk+1bZ5rcyCe4HxjnEF486jwG+rSYXg/klYKn6V2yV67Ec3wZ8YYThM5kcIj+U9c1z3z1UpvfHncDLkxyVZDPwr5icVpiXrv0Klve+chuweeE9MckTk3QdWVvSkRawbwGeMA5pvgs4t6q+tsQys/gvwCeY7JS3TpX/InDe6MesP+N326jrFuCpwEWj/HImh49vWWmFVfV5Jocy9zD5MsFi83wReBuTc1YfYHL96QWXAG/NHL7k1KWqPs2kn59k8hr9H+ALc6h6J5NDoR8fh+C+Cuwcpx7OBa5IciOTQ7kH+9LPq4DXjMOWN7P0bykfrM0bmATtrUzOxS+cSngK8GejHx8Bfnk1K7oMK12P5Xg/kyMotzD50st1h7i+ee67h8QB+/NzmRzluAH4IJNz//fOsa3H7FdV9Zk51b2c96WvM/mw/1tju7ueyamKNeGVnA5TSf4X8Jmqevta90WSjkQG7GEoyW4mhwZf2DxClyQdhAErSVKDI+0crCRJh4QBK0lSAwNWkqQGBqwkSQ0MWEmSGhiwkiQ1+P+yEZBEa3sOqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_words = {k: v for k, v in sorted(count_words.items(), key=lambda item: item[1])}\n",
    "x = list(count_words.keys())[-15:]\n",
    "y = [count_words[i] for i in x]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased\")\n",
    "model2 = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(epoch)\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.005373006220906973, 'start': 151, 'end': 162, 'answer': 'janissaries'}\n",
      "{'score': 0.0004604584537446499, 'start': 162, 'end': 174, 'answer': '.  Recruited'}\n",
      "{'score': 0.21019193530082703, 'start': 190, 'end': 211, 'answer': 'Chris-tian population'}\n",
      "an elite guard\n",
      "{'score': 0.0025214729830622673, 'start': 217, 'end': 226, 'answer': 'Hand axes'}\n",
      "{'score': 0.0002752182481344789, 'start': 310, 'end': 344, 'answer': '. Hand axes eventually were set in'}\n",
      "{'score': 0.9793450832366943, 'start': 477, 'end': 483, 'answer': 'spears'}\n",
      "spears\n",
      "{'score': 0.015940193086862564, 'start': 89, 'end': 96, 'answer': 'Assyria'}\n",
      "{'score': 0.00109293672721833, 'start': 114, 'end': 173, 'answer': '. The Sumerians were the creators of the first Mesopotamian'}\n",
      "{'score': 0.7701165676116943, 'start': 116, 'end': 129, 'answer': 'The Sumerians'}\n",
      "Sumerians\n",
      "{'score': 0.0008520749979652464, 'start': 285, 'end': 324, 'answer': 'Wellington and suffered a bloody defeat'}\n",
      "{'score': 0.0001992827601497993, 'start': 324, 'end': 374, 'answer': '.  This time,  the victorious allies exiled him to'}\n",
      "{'score': 0.49261146783828735, 'start': 179, 'end': 198, 'answer': 'Waterloo in Belgium'}\n",
      "Waterloo in Belgium\n",
      "{'score': 0.002263786504045129, 'start': 151, 'end': 165, 'answer': 'Einsatzgruppen'}\n",
      "{'score': 0.00011643364268820733, 'start': 248, 'end': 283, 'answer': 'Polish Jews and put them in ghettos'}\n",
      "{'score': 0.9937634468078613, 'start': 151, 'end': 165, 'answer': 'Einsatzgruppen'}\n",
      "Einsatzgruppen\n",
      "{'score': 0.0017571793869137764, 'start': 59, 'end': 70, 'answer': 'Nationalism'}\n",
      "{'score': 0.00016651967598591, 'start': 337, 'end': 387, 'answer': '.   Russians   crushed   the   Polish   attempt to'}\n",
      "{'score': 0.9935927987098694, 'start': 59, 'end': 70, 'answer': 'Nationalism'}\n",
      "Nationalism\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "myModel = pipeline(task = 'question-answering', model = model, tokenizer = tokenizer)\n",
    "baseModel = pipeline(task = 'question-answering', model = model2, tokenizer = tokenizer)\n",
    "goodModel = pipeline(task = 'question-answering')\n",
    "\n",
    "myModelCorrect = 0\n",
    "goodModelCorrect = 0\n",
    "for i in range(n):\n",
    "    myResults = myModel(question = val_questions[i], context = val_contexts[i])\n",
    "    goodResults = goodModel(question = val_questions[i], context = val_contexts[i])\n",
    "    print(myResults)\n",
    "    print(baseModel(question = val_questions[i], context = val_contexts[i]))\n",
    "    print(goodResults)\n",
    "    print(val_answers[i]['text'])\n",
    "    if myResults['answer'] == val_answers[i]['text']:\n",
    "        myModelCorrect += 1\n",
    "    if goodResults['answer'] == val_answers[i]['text']:\n",
    "        goodModelCorrect += 1\n",
    "\n",
    "print(myModelCorrect)\n",
    "print(goodModelCorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.009962261654436588, 'start': 8, 'end': 77, 'answer': 'Mahal is widely considered to be the most beautiful building in India'}\n",
      "{'score': 0.0018205548403784633, 'start': 105, 'end': 106, 'answer': '.'}\n",
      "{'score': 0.7801871299743652, 'start': 0, 'end': 13, 'answer': 'The Taj Mahal'}\n",
      "Taj Mahal\n",
      "{'score': 0.007324736565351486, 'start': 49, 'end': 59, 'answer': 'Çatalhüyük'}\n",
      "{'score': 0.000472668296424672, 'start': 207, 'end': 276, 'answer': 'such as weapons and jewelry and traded them with neighboring peoples.'}\n",
      "{'score': 0.9925965666770935, 'start': 161, 'end': 169, 'answer': 'artisans'}\n",
      "artisans\n",
      "{'score': 0.0026158979162573814, 'start': 99, 'end': 131, 'answer': 'James Watt,  a Scottish engineer'}\n",
      "{'score': 0.0004463562509045005, 'start': 261, 'end': 339, 'answer': '.  Before long,  cotton mills using steam engines were found all over Britain.'}\n",
      "{'score': 0.9936771988868713, 'start': 99, 'end': 109, 'answer': 'James Watt'}\n",
      "James Watt\n",
      "{'score': 0.009984852746129036, 'start': 106, 'end': 114, 'answer': 'Beauvoir'}\n",
      "{'score': 0.0004945468972437084, 'start': 130, 'end': 131, 'answer': '.'}\n",
      "{'score': 0.9754433035850525, 'start': 96, 'end': 114, 'answer': 'Simone de Beauvoir'}\n",
      "Simone de Beauvoir\n",
      "{'score': 0.008827288635075092, 'start': 124, 'end': 132, 'answer': 'Safavids'}\n",
      "{'score': 0.00032497860956937075, 'start': 264, 'end': 265, 'answer': '.'}\n",
      "{'score': 0.5623316168785095, 'start': 266, 'end': 280, 'answer': 'The Shia faith'}\n",
      "The Shia faith\n",
      "{'score': 0.0032907237764447927, 'start': 48, 'end': 60, 'answer': 'Homo sapiens'}\n",
      "{'score': 0.0002523891453165561, 'start': 160, 'end': 215, 'answer': '.  Physical evidence suggests that Homo sapiens sapiens'}\n",
      "{'score': 0.45758190751075745, 'start': 195, 'end': 215, 'answer': 'Homo sapiens sapiens'}\n",
      "Homo sapiens sapiens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.007537568919360638, 'start': 0, 'end': 11, 'answer': 'The country'}\n",
      "{'score': 0.0004055129538755864, 'start': 252, 'end': 261, 'answer': 'textile ('}\n",
      "{'score': 0.6006569266319275, 'start': 170, 'end': 185, 'answer': 'farm population'}\n",
      "farm population\n",
      "{'score': 0.015033536590635777, 'start': 89, 'end': 96, 'answer': 'Assyria'}\n",
      "{'score': 0.0009201286011375487, 'start': 120, 'end': 173, 'answer': 'Sumerians were the creators of the first Mesopotamian'}\n",
      "{'score': 0.7701165676116943, 'start': 116, 'end': 129, 'answer': 'The Sumerians'}\n",
      "Sumerians\n",
      "{'score': 0.0022297105751931667, 'start': 207, 'end': 224, 'answer': 'Thirtyfour people'}\n",
      "{'score': 0.00014074842329137027, 'start': 172, 'end': 197, 'answer': 'the Watts district of Los'}\n",
      "{'score': 0.9928851127624512, 'start': 274, 'end': 278, 'answer': '1968'}\n",
      "1968\n",
      "{'score': 0.006491181440651417, 'start': 252, 'end': 262, 'answer': 'Khrushchev'}\n",
      "{'score': 0.00016888811660464853, 'start': 252, 'end': 302, 'answer': 'Khrushchev began to place nuclear missiles in Cuba'}\n",
      "{'score': 0.9896692037582397, 'start': 278, 'end': 294, 'answer': 'nuclear missiles'}\n",
      "nuclear missiles\n",
      "{'score': 0.0033722766675055027, 'start': 195, 'end': 215, 'answer': 'Homo sapiens sapiens'}\n",
      "{'score': 0.00012560961476992816, 'start': 126, 'end': 199, 'answer': 'an anatomy similar to people today.  Physical evidence suggests that Homo'}\n",
      "{'score': 0.45758190751075745, 'start': 195, 'end': 215, 'answer': 'Homo sapiens sapiens'}\n",
      "Homo sapiens sapiens\n",
      "{'score': 0.00715132150799036, 'start': 4, 'end': 13, 'answer': 'Taj Mahal'}\n",
      "{'score': 0.0008199200965464115, 'start': 79, 'end': 99, 'answer': 'if not in the entire'}\n",
      "{'score': 0.7801871299743652, 'start': 0, 'end': 13, 'answer': 'The Taj Mahal'}\n",
      "Taj Mahal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.004106559325009584, 'start': 124, 'end': 136, 'answer': 'Fort William'}\n",
      "{'score': 0.0003441997105255723, 'start': 6, 'end': 74, 'answer': 'fighting the French, Clive was also consolidating British control in'}\n",
      "{'score': 0.8999214768409729, 'start': 217, 'end': 238, 'answer': 'an underground prison'}\n",
      "an underground prison\n",
      "{'score': 0.0066388268023729324, 'start': 51, 'end': 56, 'answer': 'Menes'}\n",
      "{'score': 0.0005916713853366673, 'start': 61, 'end': 122, 'answer': '•NEEZ) united Upper and Lower Egypt into a single kingdom and'}\n",
      "{'score': 0.6506887674331665, 'start': 156, 'end': 165, 'answer': 'A dynasty'}\n",
      "dynasty\n",
      "{'score': 0.001819412223994732, 'start': 214, 'end': 274, 'answer': 'Great Britain, Italy, Denmark, Norway, Portugal, and Iceland'}\n",
      "{'score': 9.598955512046814e-05, 'start': 589, 'end': 619, 'answer': 'formal military alliance known'}\n",
      "{'score': 0.7267914414405823, 'start': 627, 'end': 638, 'answer': 'Warsaw Pact'}\n",
      "Warsaw Pact\n",
      "{'score': 0.0047072069719433784, 'start': 190, 'end': 228, 'answer': 'Chris-tian population, the janissaries'}\n",
      "{'score': 0.0004232906794641167, 'start': 217, 'end': 281, 'answer': 'janissaries were converted to Islam. Trained as foot soldiers or'}\n",
      "{'score': 0.21019193530082703, 'start': 190, 'end': 211, 'answer': 'Chris-tian population'}\n",
      "an elite guard\n",
      "{'score': 0.0023924699053168297, 'start': 7, 'end': 20, 'answer': 'Eastern Front'}\n",
      "{'score': 0.00011700944742187858, 'start': 223, 'end': 232, 'answer': 'should be'}\n",
      "{'score': 0.6708678007125854, 'start': 524, 'end': 534, 'answer': 'Stalingrad'}\n",
      "Stalingrad\n",
      "{'score': 0.01566021516919136, 'start': 169, 'end': 175, 'answer': 'Mehmed'}\n",
      "{'score': 0.0008673588745296001, 'start': 47, 'end': 70, 'answer': 'moved to end the Byzan-'}\n",
      "{'score': 0.9922487735748291, 'start': 190, 'end': 204, 'answer': 'Constantinople'}\n",
      "Constantinople\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.0032728298101574183, 'start': 142, 'end': 157, 'answer': 'Anthropologists'}\n",
      "{'score': 0.0003849268250633031, 'start': 182, 'end': 220, 'answer': 'fossils to create a picture of peoples'}\n",
      "{'score': 0.9974045753479004, 'start': 142, 'end': 157, 'answer': 'Anthropologists'}\n",
      "Anthropologists\n",
      "{'score': 0.0017523154383525252, 'start': 101, 'end': 180, 'answer': 'British and Belgian cities were home to many industries.  With the steam engine'}\n",
      "{'score': 0.00031207280699163675, 'start': 265, 'end': 291, 'answer': 'to the cities to find work'}\n",
      "{'score': 0.5575182437896729, 'start': 0, 'end': 25, 'answer': 'European cities and towns'}\n",
      "cities and towns\n",
      "{'score': 0.06350230425596237, 'start': 4, 'end': 13, 'answer': 'Sumerians'}\n",
      "{'score': 0.0017913678893819451, 'start': 88, 'end': 106, 'answer': 'greatest invention'}\n",
      "{'score': 0.6172783970832825, 'start': 111, 'end': 124, 'answer': 'their writing'}\n",
      "writing\n",
      "{'score': 0.0014046969590708613, 'start': 99, 'end': 131, 'answer': 'James Watt,  a Scottish engineer'}\n",
      "{'score': 0.00022693948994856328, 'start': 32, 'end': 89, 'answer': 'more productive when the steam engine was improved in the'}\n",
      "{'score': 0.9936771988868713, 'start': 99, 'end': 109, 'answer': 'James Watt'}\n",
      "James Watt\n",
      "{'score': 0.008580021560192108, 'start': 188, 'end': 198, 'answer': 'Mendeleyev'}\n",
      "{'score': 0.00024076277622953057, 'start': 327, 'end': 347, 'answer': 'Faraday put together'}\n",
      "{'score': 0.4604066014289856, 'start': 173, 'end': 198, 'answer': 'Russian Dmitry Mendeleyev'}\n",
      "Dmitry Mendeleyev\n",
      "{'score': 0.002434926340356469, 'start': 106, 'end': 114, 'answer': 'Beauvoir'}\n",
      "{'score': 0.00029216802795417607, 'start': 0, 'end': 36, 'answer': 'Of great importance to the emergence'}\n",
      "{'score': 0.9754433035850525, 'start': 96, 'end': 114, 'answer': 'Simone de Beauvoir'}\n",
      "Simone de Beauvoir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.025739619508385658, 'start': 89, 'end': 96, 'answer': 'Assyria'}\n",
      "{'score': 0.0008085398585535586, 'start': 114, 'end': 173, 'answer': '. The Sumerians were the creators of the first Mesopotamian'}\n",
      "{'score': 0.7701165676116943, 'start': 116, 'end': 129, 'answer': 'The Sumerians'}\n",
      "Sumerians\n",
      "{'score': 0.0026339967735111713, 'start': 48, 'end': 68, 'answer': 'Homo sapiens sapiens'}\n",
      "{'score': 0.00016876761219464242, 'start': 246, 'end': 247, 'answer': ','}\n",
      "{'score': 0.45758190751075745, 'start': 195, 'end': 215, 'answer': 'Homo sapiens sapiens'}\n",
      "Homo sapiens sapiens\n",
      "{'score': 0.0029757090378552675, 'start': 252, 'end': 262, 'answer': 'Khrushchev'}\n",
      "{'score': 0.0002139730640919879, 'start': 252, 'end': 262, 'answer': 'Khrushchev'}\n",
      "{'score': 0.9896692037582397, 'start': 278, 'end': 294, 'answer': 'nuclear missiles'}\n",
      "nuclear missiles\n",
      "{'score': 0.004892421420663595, 'start': 33, 'end': 61, 'answer': 'Sumerian city was the temple'}\n",
      "{'score': 0.0010581574169918895, 'start': 72, 'end': 99, 'answer': 'to the chief god or goddess'}\n",
      "{'score': 0.7949788570404053, 'start': 179, 'end': 187, 'answer': 'ziggurat'}\n",
      "ziggurat\n",
      "{'score': 0.0011353063164278865, 'start': 285, 'end': 295, 'answer': 'Wellington'}\n",
      "{'score': 0.00021822746202815324, 'start': 207, 'end': 229, 'answer': '18, 1815, Napoleon met'}\n",
      "{'score': 0.49261146783828735, 'start': 179, 'end': 198, 'answer': 'Waterloo in Belgium'}\n",
      "Waterloo in Belgium\n",
      "{'score': 0.0060890354216098785, 'start': 99, 'end': 109, 'answer': 'James Watt'}\n",
      "{'score': 0.00021800810645800084, 'start': 123, 'end': 142, 'answer': 'engineer.  In 1782,'}\n",
      "{'score': 0.9936771988868713, 'start': 99, 'end': 109, 'answer': 'James Watt'}\n",
      "James Watt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.002165928715839982, 'start': 169, 'end': 177, 'answer': 'Suleyman'}\n",
      "{'score': 0.0003278833464719355, 'start': 137, 'end': 177, 'answer': '.) It may also have been during Suleyman'}\n",
      "{'score': 0.9589871168136597, 'start': 48, 'end': 58, 'answer': 'Suleyman I'}\n",
      "Suleyman I\n",
      "{'score': 0.002684706589207053, 'start': 122, 'end': 157, 'answer': 'feed a large population. As farming'}\n",
      "{'score': 0.0009512819233350456, 'start': 145, 'end': 209, 'answer': '. As farming became abundant, more people would live in the city'}\n",
      "{'score': 0.817771852016449, 'start': 37, 'end': 50, 'answer': 'river valleys'}\n",
      "river valleys\n",
      "{'score': 0.0027359866071492434, 'start': 249, 'end': 292, 'answer': 'Mesoamericans grew beans, squash, and maize'}\n",
      "{'score': 0.0002858348307199776, 'start': 220, 'end': 273, 'answer': '. In the Western Hemisphere, Mesoamericans grew beans'}\n",
      "{'score': 0.9977545738220215, 'start': 249, 'end': 262, 'answer': 'Mesoamericans'}\n",
      "Mesoamericans\n",
      "{'score': 0.00302615063264966, 'start': 311, 'end': 317, 'answer': 'Esmail'}\n",
      "{'score': 0.00017263474001083523, 'start': 437, 'end': 472, 'answer': '. Like the Ottoman sultan, the shah'}\n",
      "{'score': 0.5623316168785095, 'start': 266, 'end': 280, 'answer': 'The Shia faith'}\n",
      "The Shia faith\n",
      "{'score': 0.0023842116352170706, 'start': 59, 'end': 69, 'answer': 'Metternich'}\n",
      "{'score': 0.00019991322187706828, 'start': 339, 'end': 353, 'answer': '.  This,  they'}\n",
      "{'score': 0.9377400875091553, 'start': 40, 'end': 69, 'answer': 'Prince Klemens von Metternich'}\n",
      "Klemens von Metternich\n",
      "{'score': 0.0008336057071574032, 'start': 236, 'end': 243, 'answer': 'Denmark'}\n",
      "{'score': 9.4300921773538e-05, 'start': 90, 'end': 148, 'answer': '. The North Atlantic Treaty Organization (NATO) was formed'}\n",
      "{'score': 0.7267914414405823, 'start': 627, 'end': 638, 'answer': 'Warsaw Pact'}\n",
      "Warsaw Pact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.006224810611456633, 'start': 194, 'end': 208, 'answer': 'Archaeologists'}\n",
      "{'score': 0.0002983764570672065, 'start': 60, 'end': 80, 'answer': 'had to follow animal'}\n",
      "{'score': 0.6795850396156311, 'start': 157, 'end': 181, 'answer': 'move from place to place'}\n",
      "follow animal migrations\n",
      "{'score': 0.0036461541894823313, 'start': 249, 'end': 262, 'answer': 'Mesoamericans'}\n",
      "{'score': 0.00024466364993713796, 'start': 215, 'end': 262, 'answer': 'China. In the Western Hemisphere, Mesoamericans'}\n",
      "{'score': 0.9977545738220215, 'start': 249, 'end': 262, 'answer': 'Mesoamericans'}\n",
      "Mesoamericans\n",
      "{'score': 0.004641832783818245, 'start': 28, 'end': 64, 'answer': 'Continental System to defeat Britain'}\n",
      "{'score': 0.00031808234052732587, 'start': 171, 'end': 226, 'answer': 'sold there. By weakening Britain economically, Napoleon'}\n",
      "{'score': 0.07511847466230392, 'start': 105, 'end': 164, 'answer': 'to stop British goods from reaching the European conti-nent'}\n",
      "stop British goods\n",
      "{'score': 0.01828858070075512, 'start': 33, 'end': 88, 'answer': 'Sumerian city was the temple dedicated to the chief god'}\n",
      "{'score': 0.0011359982891008258, 'start': 179, 'end': 187, 'answer': 'ziggurat'}\n",
      "{'score': 0.7949788570404053, 'start': 179, 'end': 187, 'answer': 'ziggurat'}\n",
      "ziggurat\n",
      "{'score': 0.0019528447883203626, 'start': 91, 'end': 101, 'answer': 'U.S. fleet'}\n",
      "{'score': 0.00014739944890607148, 'start': 540, 'end': 587, 'answer': 'up arms. The United States joined with European'}\n",
      "{'score': 0.576878547668457, 'start': 374, 'end': 396, 'answer': 'attack on Pearl Harbor'}\n",
      "The attack on Pearl Harbor\n",
      "{'score': 0.006629705894738436, 'start': 252, 'end': 262, 'answer': 'Khrushchev'}\n",
      "{'score': 0.0002057638339465484, 'start': 322, 'end': 346, 'answer': 'meant to counteract U.S.'}\n",
      "{'score': 0.9896692037582397, 'start': 278, 'end': 294, 'answer': 'nuclear missiles'}\n",
      "nuclear missiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.014724867418408394, 'start': 188, 'end': 198, 'answer': 'Mendeleyev'}\n",
      "{'score': 0.00022350177459884435, 'start': 52, 'end': 56, 'answer': 'germ'}\n",
      "{'score': 0.4604066014289856, 'start': 173, 'end': 198, 'answer': 'Russian Dmitry Mendeleyev'}\n",
      "Dmitry Mendeleyev\n",
      "{'score': 0.0025052528362721205, 'start': 402, 'end': 449, 'answer': 'Wordsworth,  the foremost English romantic poet'}\n",
      "{'score': 0.00027407798916101456, 'start': 384, 'end': 412, 'answer': 'poetry of William Wordsworth'}\n",
      "{'score': 0.9205650687217712, 'start': 75, 'end': 81, 'answer': 'poetry'}\n",
      "poetry\n",
      "{'score': 0.02752692624926567, 'start': 33, 'end': 61, 'answer': 'Sumerian city was the temple'}\n",
      "{'score': 0.0011059037642553449, 'start': 33, 'end': 41, 'answer': 'Sumerian'}\n",
      "{'score': 0.7949788570404053, 'start': 179, 'end': 187, 'answer': 'ziggurat'}\n",
      "ziggurat\n",
      "{'score': 0.002084980020299554, 'start': 35, 'end': 90, 'answer': 'Cold War led to the formation of new military alliances'}\n",
      "{'score': 7.788523362250999e-05, 'start': 461, 'end': 468, 'answer': 'joined.'}\n",
      "{'score': 0.7267914414405823, 'start': 627, 'end': 638, 'answer': 'Warsaw Pact'}\n",
      "Warsaw Pact\n",
      "{'score': 0.035719290375709534, 'start': 17, 'end': 25, 'answer': 'Safavids'}\n",
      "{'score': 0.0025927771348506212, 'start': 87, 'end': 146, 'answer': 'elements affected virtually all aspects of Safavid society.'}\n",
      "{'score': 0.9103766083717346, 'start': 79, 'end': 86, 'answer': 'Persian'}\n",
      "Persian\n",
      "{'score': 0.00956977903842926, 'start': 252, 'end': 262, 'answer': 'Khrushchev'}\n",
      "{'score': 0.00019444503413978964, 'start': 191, 'end': 250, 'answer': 'Pigs, the Soviet Union sent advisers to Cuba. Then, in 1962'}\n",
      "{'score': 0.9896692037582397, 'start': 278, 'end': 294, 'answer': 'nuclear missiles'}\n",
      "nuclear missiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.0019034704891964793, 'start': 60, 'end': 66, 'answer': 'Yahweh'}\n",
      "{'score': 0.00018479430582374334, 'start': 278, 'end': 283, 'answer': ', the'}\n",
      "{'score': 0.9355998039245605, 'start': 60, 'end': 66, 'answer': 'Yahweh'}\n",
      "monotheistic\n",
      "{'score': 0.004282507114112377, 'start': 151, 'end': 164, 'answer': 'Ludwig Erhard'}\n",
      "{'score': 0.0001953436149051413, 'start': 166, 'end': 178, 'answer': 'Unemployment'}\n",
      "{'score': 0.9262884259223938, 'start': 16, 'end': 28, 'answer': 'West Germany'}\n",
      "West Germany\n",
      "{'score': 0.008042612113058567, 'start': 33, 'end': 41, 'answer': 'Sumerian'}\n",
      "{'score': 0.0010778330033645034, 'start': 85, 'end': 145, 'answer': 'god or goddess of the city. This temple was often built atop'}\n",
      "{'score': 0.7949788570404053, 'start': 179, 'end': 187, 'answer': 'ziggurat'}\n",
      "ziggurat\n",
      "{'score': 0.03229561075568199, 'start': 17, 'end': 25, 'answer': 'Safavids'}\n",
      "{'score': 0.0017660919111222029, 'start': 13, 'end': 74, 'answer': 'the Safavids was a mixed society.  The combination of Turkish'}\n",
      "{'score': 0.9103766083717346, 'start': 79, 'end': 86, 'answer': 'Persian'}\n",
      "Persian\n",
      "{'score': 0.005945318844169378, 'start': 49, 'end': 59, 'answer': 'Çatalhüyük'}\n",
      "{'score': 0.00031209178268909454, 'start': 268, 'end': 281, 'answer': 'peoples. This'}\n",
      "{'score': 0.9925965666770935, 'start': 161, 'end': 169, 'answer': 'artisans'}\n",
      "artisans\n",
      "{'score': 0.002851237775757909, 'start': 104, 'end': 142, 'answer': 'Heydrich created special strike forces'}\n",
      "{'score': 0.0001255100651178509, 'start': 396, 'end': 431, 'answer': 'unsanitary housing. The Nazis tried'}\n",
      "{'score': 0.9937634468078613, 'start': 151, 'end': 165, 'answer': 'Einsatzgruppen'}\n",
      "Einsatzgruppen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "DONE\n",
      "{'score': 0.015073477290570736, 'start': 64, 'end': 83, 'answer': 'Byzan-tine   Empire'}\n",
      "{'score': 0.0007856804295442998, 'start': 190, 'end': 205, 'answer': 'Constantinople.'}\n",
      "{'score': 0.9922487735748291, 'start': 190, 'end': 204, 'answer': 'Constantinople'}\n",
      "Constantinople\n",
      "{'score': 0.0665799006819725, 'start': 4, 'end': 13, 'answer': 'Sumerians'}\n",
      "{'score': 0.0013467747485265136, 'start': 82, 'end': 106, 'answer': 'their greatest invention'}\n",
      "{'score': 0.6172783970832825, 'start': 111, 'end': 124, 'answer': 'their writing'}\n",
      "writing\n",
      "{'score': 0.002726909937337041, 'start': 48, 'end': 56, 'answer': 'Suleyman'}\n",
      "{'score': 0.00022503432410303503, 'start': 213, 'end': 271, 'answer': 'began to occur, however. Having executed his two most able'}\n",
      "{'score': 0.9589871168136597, 'start': 48, 'end': 58, 'answer': 'Suleyman I'}\n",
      "Suleyman I\n",
      "{'score': 0.002722371369600296, 'start': 0, 'end': 69, 'answer': 'The country became a single massive market for the manufactured goods'}\n",
      "{'score': 0.0003982288180850446, 'start': 357, 'end': 387, 'answer': ',  to work in their factories.'}\n",
      "{'score': 0.6006569266319275, 'start': 170, 'end': 185, 'answer': 'farm population'}\n",
      "farm population\n",
      "{'score': 0.015149224549531937, 'start': 98, 'end': 129, 'answer': 'Akkad, and Sumer. The Sumerians'}\n",
      "{'score': 0.0007833773270249367, 'start': 139, 'end': 173, 'answer': 'creators of the first Mesopotamian'}\n",
      "{'score': 0.7701165676116943, 'start': 116, 'end': 129, 'answer': 'The Sumerians'}\n",
      "Sumerians\n",
      "{'score': 0.018659906461834908, 'start': 155, 'end': 164, 'answer': 'Artifacts'}\n",
      "{'score': 0.0007039371412247419, 'start': 113, 'end': 164, 'answer': 'examine artifacts—objects made by humans. Artifacts'}\n",
      "{'score': 0.944131076335907, 'start': 121, 'end': 130, 'answer': 'artifacts'}\n",
      "artifacts\n",
      "6\n",
      "1\n",
      "43\n",
      "0.009452289873540092\n",
      "0.0005253525706696867\n",
      "0.7826526670406262\n"
     ]
    }
   ],
   "source": [
    "myModelCorrect = 0\n",
    "baseModelCorrect = 0\n",
    "goodModelCorrect = 0\n",
    "\n",
    "myModelScore = 0\n",
    "baseModelScore = 0\n",
    "goodModelScore = 0\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    data = []\n",
    "\n",
    "    for i in question_answer_data:\n",
    "        dat = []\n",
    "        answer = {}\n",
    "        dat.append(question_answer_data[i]['context'])\n",
    "        dat.append(question_answer_data[i]['question'])\n",
    "        answer['text'] = question_answer_data[i]['answer']\n",
    "        answer_start = question_answer_data[i]['context'].find(answer['text']);\n",
    "        answer[\"answer_start\"] = answer_start\n",
    "        answer[\"answer_end\"] = answer_start + len(question_answer_data[i]['answer'])\n",
    "        dat.append(answer)\n",
    "        data.append(dat)\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    val_contexts = []\n",
    "    val_questions = []\n",
    "    val_answers = []\n",
    "\n",
    "    train_contexts = []\n",
    "    train_questions = []\n",
    "    train_answers = []\n",
    "\n",
    "    n = 6\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if i < n:\n",
    "            val_contexts.append(data[i][0])\n",
    "            val_questions.append(data[i][1])\n",
    "            val_answers.append(data[i][2])\n",
    "        else:\n",
    "            train_contexts.append(data[i][0])\n",
    "            train_questions.append(data[i][1])\n",
    "            train_answers.append(data[i][2])\n",
    "    \n",
    "    add_end_idx(train_answers, train_contexts)\n",
    "    add_end_idx(val_answers, val_contexts)\n",
    "    \n",
    "    train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
    "    \n",
    "    add_token_positions(train_encodings, train_answers)\n",
    "    add_token_positions(val_encodings, val_answers)\n",
    "    \n",
    "    train_dataset = SquadDataset(train_encodings)\n",
    "    val_dataset = SquadDataset(val_encodings)\n",
    "    \n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased\")\n",
    "    model2 = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased\")\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        print(epoch)\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "    print(\"DONE\")\n",
    "    \n",
    "    myModel = pipeline(task = 'question-answering', model = model, tokenizer = tokenizer)\n",
    "    baseModel = pipeline(task = 'question-answering', model = model2, tokenizer = tokenizer)\n",
    "    goodModel = pipeline(task = 'question-answering')\n",
    "    \n",
    "    for i in range(n):\n",
    "        myResults = myModel(question = val_questions[i], context = val_contexts[i])\n",
    "        goodResults = goodModel(question = val_questions[i], context = val_contexts[i])\n",
    "        baseResults = baseModel(question = val_questions[i], context = val_contexts[i])\n",
    "        \n",
    "        myModelScore += myResults['score']\n",
    "        baseModelScore += baseResults['score']\n",
    "        goodModelScore += goodResults['score']\n",
    "        \n",
    "        print(myResults)\n",
    "        print(baseResults)\n",
    "        print(goodResults)\n",
    "        print(val_answers[i]['text'])\n",
    "        if myResults['answer'] == val_answers[i]['text']:\n",
    "            myModelCorrect += 1\n",
    "        if baseResults['answer'] == val_answers[i]['text']:\n",
    "            baseModelCorrect += 1\n",
    "        if goodResults['answer'] == val_answers[i]['text']:\n",
    "            goodModelCorrect += 1\n",
    "            \n",
    "            \n",
    "print(myModelCorrect)\n",
    "print(baseModelCorrect)\n",
    "print(goodModelCorrect)\n",
    "\n",
    "print(myModelScore/60)\n",
    "print(baseModelScore/60)\n",
    "print(goodModelScore/60)            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 scores:\n",
    "\n",
    "myModelCorrect = 10 <br>\n",
    "baseModelCorrect = 0 <br>\n",
    "goodModelCorrect = 38 <br>\n",
    "\n",
    "Accuracy scores:\n",
    "\n",
    "myModelCorrect = 18 <br>\n",
    "baseModelCorrect = 6 <br>\n",
    "goodModelCorrect = 51 <br>\n",
    "\n",
    "Percent scores:\n",
    "\n",
    "myModelPercentages = 0.009452289873540092 <br>\n",
    "baseModelPercentages = 0.0005253525706696867 <br>\n",
    "goodModelPercentages = 0.7826526670406262 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Distil Bert Trained', 'Distil Bert Base', 'Distil Bert SQuAD']\n",
    "f1_scores = [10/60,0/60,38/60]\n",
    "accurancy_scores = [18/60,6/60,51/60]\n",
    "percent_scores = [0.009452289873540092,0.0005253525706696867,0.7826526670406262]\n",
    "\n",
    "fig_f1 = plt.figure()\n",
    "ax = fig_f1.add_axes([0,0,1,1])\n",
    "ax.bar(models,f1_scores)\n",
    "ax.set_ylabel('F1 correct')\n",
    "ax.set_title('F1 scores by model')\n",
    "plt.show()\n",
    "\n",
    "fig_accuracy = plt.figure()\n",
    "ax = fig_accuracy.add_axes([0,0,1,1])\n",
    "ax.bar(models,accurancy_scores)\n",
    "ax.set_ylabel('Accuracy correct')\n",
    "ax.set_title('Accuracy scores by model')\n",
    "plt.show()\n",
    "\n",
    "fig_percent = plt.figure()\n",
    "ax = fig_percent.add_axes([0,0,1,1])\n",
    "ax.bar(models,percent_scores)\n",
    "ax.set_ylabel('Percent confident')\n",
    "ax.set_title('Confidence in answer by model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
